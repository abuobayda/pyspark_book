{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.6.tgz\n",
    "!tar xvf spark-2.4.4-bin-hadoop2.6.tgz\n",
    "!pip install -q findspark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.6\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"PySpark_dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicte the change of Infant Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lVG8zgYn1RwZ",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spark = SparkSession.builder.master('local[2]') \\\n",
    "        .appName('ML_infant_app') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzVMMcIU1K-U"
   },
   "source": [
    "First, we load the data with the help of the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nnpJUzq0yqC"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ\n",
    "labels = [\n",
    "    ('INFANT_ALIVE_AT_REPORT', typ.IntegerType()),\n",
    "    ('BIRTH_PLACE', typ.StringType()),\n",
    "    ('MOTHER_AGE_YEARS', typ.IntegerType()),\n",
    "    ('FATHER_COMBINED_AGE', typ.IntegerType()),\n",
    "    ('CIG_BEFORE', typ.IntegerType()),\n",
    "    ('CIG_1_TRI', typ.IntegerType()),\n",
    "    ('CIG_2_TRI', typ.IntegerType()),\n",
    "    ('CIG_3_TRI', typ.IntegerType()),\n",
    "    ('MOTHER_HEIGHT_IN', typ.IntegerType()),\n",
    "    ('MOTHER_PRE_WEIGHT', typ.IntegerType()),\n",
    "    ('MOTHER_DELIVERY_WEIGHT', typ.IntegerType()),\n",
    "    ('MOTHER_WEIGHT_GAIN', typ.IntegerType()),\n",
    "    ('DIABETES_PRE', typ.IntegerType()),\n",
    "    ('DIABETES_GEST', typ.IntegerType()),\n",
    "    ('HYP_TENS_PRE', typ.IntegerType()),\n",
    "    ('HYP_TENS_GEST', typ.IntegerType()),\n",
    "    ('PREV_BIRTH_PRETERM', typ.IntegerType())\n",
    "]\n",
    "schema = typ.StructType([\n",
    "    typ.StructField(e[0], e[1], False) for e in labels\n",
    "])\n",
    "births = spark.read.csv('births_transformed.csv.gz', \n",
    "                        header=True, \n",
    "                        schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "colab_type": "code",
    "id": "HWtk5-tV1bS4",
    "outputId": "473345bf-636f-4c08-ef63-a970457d4ed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- INFANT_ALIVE_AT_REPORT: integer (nullable = true)\n",
      " |-- BIRTH_PLACE: string (nullable = true)\n",
      " |-- MOTHER_AGE_YEARS: integer (nullable = true)\n",
      " |-- FATHER_COMBINED_AGE: integer (nullable = true)\n",
      " |-- CIG_BEFORE: integer (nullable = true)\n",
      " |-- CIG_1_TRI: integer (nullable = true)\n",
      " |-- CIG_2_TRI: integer (nullable = true)\n",
      " |-- CIG_3_TRI: integer (nullable = true)\n",
      " |-- MOTHER_HEIGHT_IN: integer (nullable = true)\n",
      " |-- MOTHER_PRE_WEIGHT: integer (nullable = true)\n",
      " |-- MOTHER_DELIVERY_WEIGHT: integer (nullable = true)\n",
      " |-- MOTHER_WEIGHT_GAIN: integer (nullable = true)\n",
      " |-- DIABETES_PRE: integer (nullable = true)\n",
      " |-- DIABETES_GEST: integer (nullable = true)\n",
      " |-- HYP_TENS_PRE: integer (nullable = true)\n",
      " |-- HYP_TENS_GEST: integer (nullable = true)\n",
      " |-- PREV_BIRTH_PRETERM: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "births.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "qRzGi5iR3xeg",
    "outputId": "72e76bd9-0877-426a-9cd5-815c822b1e56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------+----------------+-------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+\n",
      "|INFANT_ALIVE_AT_REPORT|BIRTH_PLACE|MOTHER_AGE_YEARS|FATHER_COMBINED_AGE|CIG_BEFORE|CIG_1_TRI|CIG_2_TRI|CIG_3_TRI|MOTHER_HEIGHT_IN|MOTHER_PRE_WEIGHT|MOTHER_DELIVERY_WEIGHT|MOTHER_WEIGHT_GAIN|DIABETES_PRE|DIABETES_GEST|HYP_TENS_PRE|HYP_TENS_GEST|PREV_BIRTH_PRETERM|\n",
      "+----------------------+-----------+----------------+-------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+\n",
      "|                     0|          1|              29|                 99|         0|        0|        0|        0|              99|              999|                   999|                99|           0|            0|           0|            0|                 0|\n",
      "|                     0|          1|              22|                 29|         0|        0|        0|        0|              65|              180|                   198|                18|           0|            0|           0|            0|                 0|\n",
      "|                     0|          1|              38|                 40|         0|        0|        0|        0|              63|              155|                   167|                12|           0|            0|           0|            0|                 0|\n",
      "+----------------------+-----------+----------------+-------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "births.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "waNmu5HB16wz"
   },
   "source": [
    "## Creating Transformers\n",
    "Before we can use the dataset to estimate a model, we need to do some transformations. Since statistical models can only operate on numeric data, we will have to encode the BIRTH_PLACE variable.\n",
    "\n",
    "To encode the BIRTH_PLACE column, we will use the OneHotEncoder method. However, the method cannot accept StringType columns; it can only deal with numeric types so first we will cast the column to an IntegerType:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-IKKW4en1hF_"
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as ft\n",
    "\n",
    "# convert Birth_place from string to integer\n",
    "births = births \\\n",
    "    .withColumn('BIRTH_PLACE_INT', births['BIRTH_PLACE'] \\\n",
    "    .cast(typ.IntegerType()))\n",
    "# create our first Transformer\n",
    "encoder = ft.OneHotEncoder(\n",
    "    inputCol='BIRTH_PLACE_INT', \n",
    "    outputCol='BIRTH_PLACE_VEC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jv4we1i74NM1"
   },
   "source": [
    "A list of all the columns to be combined together to form the outputColâ€”the 'features'. Note that we use the output of the encoder object (by calling the .getOutputCol() method), so we do not have to remember to change this parameter's value should we change the name of the output column in the encoder object at any point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FfGJk1Q139db"
   },
   "outputs": [],
   "source": [
    "featuresCreator = ft.VectorAssembler(\n",
    "    inputCols=[\n",
    "        col[0] \n",
    "        for col \n",
    "        in labels[2:]] + \\\n",
    "    [encoder.getOutputCol()], \n",
    "    outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xCAzHKif4XqX"
   },
   "source": [
    "## Creating an estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hvbaVIb4a3j"
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.classification as cl\n",
    "# create the logistic regression model\n",
    "logistic = cl.LogisticRegression(\n",
    "    maxIter=10, \n",
    "    regParam=0.01, \n",
    "    labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rH1C_ehm6Gal"
   },
   "source": [
    "We would not have to specify the labelCol parameter if our target column had the name 'label'. Also, if the output of our featuresCreator was not called 'features', we would have to specify the featuresCol by (most conveniently) calling the getOutputCol() method on the featuresCreator object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdq30AC44nro"
   },
   "source": [
    "## Creating a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "40D2zbPI6Qu7"
   },
   "source": [
    "![alt text](https://learning.oreilly.com/library/view/learning-pyspark/9781786463708/graphics/B05793_06_01.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sm6qrlXd4qjZ"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "        encoder, \n",
    "        featuresCreator, \n",
    "        logistic\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KxrkP8OB4xtw"
   },
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o96qFEDJ6bfm"
   },
   "source": [
    "Before you fit the model, we need to split our dataset into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJtWjVR54zz4"
   },
   "outputs": [],
   "source": [
    "births_train, births_test = births.randomSplit([0.7, 0.3], seed=666)\n",
    "# train, test, val = births.randomSplit([0.7, 0.2, 0.1], seed=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-E2Cc1D43yR"
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(births_train)\n",
    "test_model = model.transform(births_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZbRBmHg86utW"
   },
   "source": [
    "The .fit(...) method of the pipeline object takes our training dataset as an input. Under the hood, the births_train dataset is passed first to the encoder object. The DataFrame that is created at the encoder stage then gets passed to the featuresCreator that creates the 'features' column. Finally, the output from this stage is passed to the logistic object that estimates the final model.\n",
    "\n",
    "The .fit(...) method returns the PipelineModel object (the model object in the preceding snippet) that can then be used for prediction; we attain this by calling the .transform(...) method and passing the testing dataset created earlier. Here's what the test_model looks like in the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "_ZT_Q84949Sn",
    "outputId": "c4528a31-adec-4f75-ad3b-f08d9d0e7df6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE='1', MOTHER_AGE_YEARS=13, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=66, MOTHER_PRE_WEIGHT=133, MOTHER_DELIVERY_WEIGHT=135, MOTHER_WEIGHT_GAIN=2, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0, BIRTH_PLACE_INT=1, BIRTH_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 13.0, 1: 99.0, 6: 66.0, 7: 133.0, 8: 135.0, 9: 2.0, 16: 1.0}), rawPrediction=DenseVector([1.0573, -1.0573]), probability=DenseVector([0.7422, 0.2578]), prediction=0.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KghoPDju7A6L"
   },
   "source": [
    "As you can see, we get all the columns from the Transfomers and Estimators. The logistic regression model outputs several columns: the rawPrediction is the value of the linear combination of features and the Î² coefficients, the probability is the calculated probability for each of the classes, and finally, the prediction is our final class assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "653BOsaB5DUz"
   },
   "source": [
    "## Evaluating the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6BbN3tY7Rw3"
   },
   "source": [
    "Obviously, we would like to now test how well our model did. PySpark exposes a number of evaluation methods for classification and regression in the .evaluation section of the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ah-nCA9u5Hle"
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='probability', \n",
    "    labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "QA5fNSUB5NdV",
    "outputId": "c7cef58a-3dd7-476f-add7-bd15f225c72e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7401301847095617\n",
      "0.7139354342365674\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(test_model, \n",
    "    {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(test_model, \n",
    "   {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1cHLOfhp7YNg"
   },
   "source": [
    "The area under the ROC of 74% and area under PR of 71% shows a well-defined model, but nothing out of extraordinary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kx6gUXZR5XIK"
   },
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hVZrfbwJ7iMd"
   },
   "source": [
    "PySpark allows you to save the Pipeline definition for later use. It not only saves the pipeline structure, but also all the definitions of all the Transformers and Estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_tFPi4l5Zfj"
   },
   "outputs": [],
   "source": [
    "pipelinePath = './infant_oneHotEncoder_Logistic_Pipeline'\n",
    "pipeline.write().overwrite().save(pipelinePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Avyv_PC7l1w"
   },
   "source": [
    "So, you can load it up later and use it straight away to .fit(...) and predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "VFYEW5jC5bFz",
    "outputId": "f7a852d0-81d9-4b19-c3ad-9f891959fbcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE='1', MOTHER_AGE_YEARS=13, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=66, MOTHER_PRE_WEIGHT=133, MOTHER_DELIVERY_WEIGHT=135, MOTHER_WEIGHT_GAIN=2, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0, BIRTH_PLACE_INT=1, BIRTH_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 13.0, 1: 99.0, 6: 66.0, 7: 133.0, 8: 135.0, 9: 2.0, 16: 1.0}), rawPrediction=DenseVector([1.0573, -1.0573]), probability=DenseVector([0.7422, 0.2578]), prediction=0.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadedPipeline = Pipeline.load(pipelinePath)\n",
    "loadedPipeline.fit(births_train).transform(births_test).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mY6vmUqQ7reC"
   },
   "source": [
    "## Parameter hyper-tuning\n",
    "A concept of parameter hyper-tuning is to find the best parameters of the model: for example, the maximum number of iterations needed to properly estimate the logistic regression model or maximum depth of a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xu1jbEOj8UbM"
   },
   "source": [
    "### Grid search\n",
    "Grid search is an exhaustive algorithm that loops through the list of defined parameter values, estimates separate models, and chooses the best one given some evaluation metric.\n",
    "\n",
    "It might take a lot of time to select the best model as the number of models to estimate would grow very quickly as the number of parameters and parameter values grow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7uuK7gs28PEu"
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "\n",
    "logistic = cl.LogisticRegression(\n",
    "    labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "\n",
    "\n",
    "grid = tune.ParamGridBuilder() \\\n",
    "    .addGrid(logistic.maxIter,  \n",
    "             [2, 10, 50]) \\\n",
    "    .addGrid(logistic.regParam, \n",
    "             [0.01, 0.05, 0.3]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhxu2fG086-q"
   },
   "source": [
    "First, we specify the model we want to optimize the parameters of. Next, we decide which parameters we will be optimizing, and what values for those parameters to test. We use the ParamGridBuilder() object from the .tuning subpackage, and keep adding the parameters to the grid with the .addGrid(...) method: the first parameter is the parameter object of the model we want to optimize (in our case, these are logistic.maxIter and logistic.regParam), and the second parameter is a list of values we want to loop through. Calling the .build() method on the .ParamGridBuilder builds the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2o-lnY598724"
   },
   "outputs": [],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='probability', \n",
    "    labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GApKdDKy9FVp"
   },
   "outputs": [],
   "source": [
    "cv = tune.CrossValidator(\n",
    "    estimator=logistic, \n",
    "    estimatorParamMaps=grid, \n",
    "    evaluator=evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gD-h2saT9JcU"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[encoder ,featuresCreator])\n",
    "data_transformer = pipeline.fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBuvQWo29JbP"
   },
   "outputs": [],
   "source": [
    "cvModel = cv.fit(data_transformer.transform(births_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "DCoM5_1E95NC",
    "outputId": "9ddb6b29-5a0e-4896-f0d7-24bb989ba72c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7404959803309813\n",
      "0.7157971108486731\n"
     ]
    }
   ],
   "source": [
    "data_train = data_transformer \\\n",
    "    .transform(births_test)\n",
    "results = cvModel.transform(data_train)\n",
    "print(evaluator.evaluate(results, \n",
    "     {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(results, \n",
    "     {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FIsAhRf_9-5v",
    "outputId": "239d5d0b-2470-443f-f8d2-823cccde52d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'maxIter': 50}, {'regParam': 0.01}], 0.7385001697378982)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [\n",
    "           (\n",
    "               [\n",
    "                {key.name: paramValue}\n",
    "                for key, paramValue\n",
    "                in zip(\n",
    "                    params.keys(),\n",
    "                    params.values())\n",
    "                ], metric\n",
    "            )\n",
    "           for params, metric \n",
    "           in zip(\n",
    "               cvModel.getEstimatorParamMaps(),\n",
    "               cvModel.avgMetrics\n",
    "               )\n",
    "]\n",
    "sorted(results,\n",
    "       key=lambda el: el[1],\n",
    "       reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "chdz6eyo_UFJ"
   },
   "source": [
    "### Train-validation splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y3JkqrSX_e23"
   },
   "source": [
    "The TrainValidationSplit model, to select the best model, performs a random split of the input dataset (the training dataset) into two subsets: smaller training and validation subsets. The split is only performed once.\n",
    "\n",
    "In this example, we will also use the ChiSqSelector to select only the top five features, thus limiting the complexity of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwMYGCCG9_qw"
   },
   "outputs": [],
   "source": [
    "selector = ft.ChiSqSelector(\n",
    "    numTopFeatures=5, \n",
    "    featuresCol=featuresCreator.getOutputCol(), \n",
    "    outputCol='selectedFeatures',\n",
    "    labelCol='INFANT_ALIVE_AT_REPORT'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fPqudpO_ko0"
   },
   "outputs": [],
   "source": [
    "logistic = cl.LogisticRegression(\n",
    "    labelCol='INFANT_ALIVE_AT_REPORT',\n",
    "    featuresCol='selectedFeatures'\n",
    ")\n",
    "pipeline = Pipeline(stages=[encoder, featuresCreator, selector])\n",
    "data_transformer = pipeline.fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sgLI0l4w_vT2"
   },
   "outputs": [],
   "source": [
    "tvs = tune.TrainValidationSplit(\n",
    "    estimator=logistic, \n",
    "    estimatorParamMaps=grid, \n",
    "    evaluator=evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "lpSxndJG_xoZ",
    "outputId": "6ddada66-6d57-499a-b7b9-649b3a4fb4a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7294296314442145\n",
      "0.703775950281647\n"
     ]
    }
   ],
   "source": [
    "tvsModel = tvs.fit(\n",
    "    data_transformer \\\n",
    "        .transform(births_train)\n",
    ")\n",
    "data_train = data_transformer \\\n",
    "    .transform(births_test)\n",
    "results = tvsModel.transform(data_train)\n",
    "print(evaluator.evaluate(results, \n",
    "     {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(results, \n",
    "     {evaluator.metricName: 'areaUnderPR'}))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Predicting the chances of infant survival with ML.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
