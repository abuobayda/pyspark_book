{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1WF6vKU1fsB4"
   },
   "source": [
    "# PySpark overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JLXZPb9qfm78"
   },
   "source": [
    "Apache Spark is written in Scala programming language. To support Python with Spark, Apache Spark Community released a tool, PySpark. Using PySpark, you can work with RDDs in Python programming language also. It is because of a library called Py4j that they are able to achieve this.\n",
    "\n",
    "PySpark offers PySpark Shell which links the Python API to the spark core and initializes the Spark context. Majority of data scientists and analytics experts today use Python because of its rich library set. Integrating Python with Spark is a boon to them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSospL1hf0ag"
   },
   "source": [
    "## Importing PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wiej9SoUfqKq"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Intro_to_pyspark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3PxoQHKgbAo"
   },
   "source": [
    "SparkContext is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. The driver program then runs the operations inside the executors on worker nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ku9NRV9zglYc"
   },
   "source": [
    "**The details of a PySpark class**\n",
    "```\n",
    "class pyspark.SparkContext (\n",
    "   # master --> It is the URL of the cluster it connects to\n",
    "   master = None,\n",
    "   # appName --> Name of your job\n",
    "   appName = None,\n",
    "   # sparkHome --> Spark installation directory\n",
    "   sparkHome = None,\n",
    "   # pyFiles --> The .zip or .py files to send to the cluster and add to the PYTHONPATH\n",
    "   pyFiles = None,\n",
    "   # Environment --> Worker nodes environment variables\n",
    "   environment = None,\n",
    "   # batchSize --> The number of Python objects represented as a single Java object. Set 1 to disable batching, \n",
    "   0 to automatically choose the batch size based on object sizes, or -1 to use an unlimited batch size.\n",
    "   batchSize = 0,\n",
    "   # Serializer --> RDD serializer\n",
    "   serializer = PickleSerializer(),\n",
    "   # Conf --> An object of L{SparkConf} to set all the Spark properties.\n",
    "   conf = None,\n",
    "   # Gateway --> Use an existing gateway and JVM, otherwise initializing a new JVM.\n",
    "   gateway = None,\n",
    "   # JSC --> The JavaSparkContext instance. \n",
    "   jsc = None,\n",
    "   # profiler_cls --> A class of custom Profiler used to do profiling (the default is pyspark.profiler.BasicProfiler).\n",
    "   profiler_cls = <class 'pyspark.profiler.BasicProfiler'>\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMaA036BfiIc"
   },
   "source": [
    "Retrieve SparkContext version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "77j2zUYwfop-",
    "outputId": "3efac232-44d4-45b9-df8a-552ee323bef8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.4'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5FrK1BfgJzq"
   },
   "source": [
    "Retrieve Python version of SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "f9F8Dz2TgRaB",
    "outputId": "c2e0f82a-7115-4a4f-d277-0abc25e340dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6'"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.pythonVer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pw6YiRV2gVul"
   },
   "source": [
    "URL of the cluster or \"local\" string to run in local mode of SparkContex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HfuD-eyJgTbI",
    "outputId": "4d173334-a389-4625-a3ec-e66918b4ce83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3EX3WSqWgkpX"
   },
   "source": [
    "# Loading data in PySpark\n",
    "SparkContext's *paralleliza()* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N3RPhDgEgf4P"
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4rb5WsH-g-vv"
   },
   "source": [
    "SparksContext's *textFile()* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QzqL0xQAg31b"
   },
   "outputs": [],
   "source": [
    "rdd2 = sc.textFile(\"sample_data/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MpNJsWg0hw8t"
   },
   "source": [
    "Your Turn:\n",
    "\n",
    "\n",
    "*   Print the version of SparkContext in the PySpark shell.\n",
    "*   Print the Python version of SparkContext in the PySpark shell.\n",
    "*   What is the master of SparkContext in the PySpark shell?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3v0RBzXKhVXp"
   },
   "outputs": [],
   "source": [
    "# Print the version of SparkContext\n",
    "print(\"The version of Spark Context in the PySpark shell is\", sc.____)\n",
    "\n",
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.____)\n",
    "\n",
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ljH69DxWiNio"
   },
   "source": [
    "* Create a python list named numb containing the numbers 1 to 100.\n",
    "* Load the list into Spark using Spark Context's parallelize method and assign it to a variable spark_data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKQ1PmqTiU-U"
   },
   "outputs": [],
   "source": [
    "# Create a python list of numbers from 1 to 100 \n",
    "numb = range(____, ____)\n",
    "\n",
    "# Load the list into PySpark  \n",
    "spark_data = sc.____(numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxRId6UXiY7i"
   },
   "source": [
    "\n",
    "\n",
    "*   Load a local text file sample_data/README.md in PySpark shell.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9o6w5iRmipnD"
   },
   "outputs": [],
   "source": [
    "file_path = 'sample_data/README.md'\n",
    "# Load a local file into PySpark shell\n",
    "lines = sc.____(file_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "1.Intro_to_pyspark.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
